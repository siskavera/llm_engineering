{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: Generative AI can create high-quality content such as:\n",
      "\t* Articles, blog posts, and social media posts\n",
      "\t* Product descriptions, product recommendations, and sales copy\n",
      "\t* Music, videos, and audio content for marketing campaigns\n",
      "2. **Image and Video Creation**: Generative AI can generate:\n",
      "\t* High-resolution images and graphics for advertising, packaging, and marketing materials\n",
      "\t* 3D models and animations for video games, film, and architecture\n",
      "\t* Product visuals, product demonstrations, and virtual try-on capabilities\n",
      "3. **Chatbots and Virtual Assistants**: Generative AI can power:\n",
      "\t* Personalized chatbots that provide customer support, answer questions, and offer recommendations\n",
      "\t* Virtual assistants for customer service, booking appointments, and managing orders\n",
      "4. **Predictive Analytics and Forecasting**: Generative AI can analyze large datasets to predict:\n",
      "\t* Customer behavior, preferences, and purchase patterns\n",
      "\t* Market trends, sales forecasts, and supply chain optimization\n",
      "5. **Customer Segmentation and Personalization**: Generative AI can help identify:\n",
      "\t* Customer segments based on demographics, behavior, and preferences\n",
      "\t* Personalized product recommendations and offers for individual customers\n",
      "6. **Automated Content Translation**: Generative AI can translate:\n",
      "\t* Large volumes of text, articles, and documents from one language to another\n",
      "7. **Music Composition and Sound Design**: Generative AI can create:\n",
      "\t* Original music tracks for film, TV, and video games\n",
      "\t* Custom sound effects, FX, and audio branding materials\n",
      "8. **Digital Art and Design**: Generative AI can generate:\n",
      "\t* Unique digital art pieces for advertising, packaging, and marketing materials\n",
      "\t* Custom graphics, icons, and illustrations for branding and design projects\n",
      "9. **Recommendation Systems**: Generative AI can develop:\n",
      "\t* Personalized product recommendations based on user behavior, preferences, and purchasing history\n",
      "10. **Risk Analysis and Credit Scoring**: Generative AI can analyze large datasets to assess:\n",
      "\t* Creditworthiness and credit risk for lending decisions\n",
      "\t* Market trends and market risk for investment decisions\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As the technology continues to evolve, we can expect to see even more innovative uses in various industries.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Generative AI has numerous business applications across various industries, including:\n",
       "\n",
       "1. **Content Generation**: AI-powered tools can generate high-quality content such as articles, social media posts, product descriptions, and more. This can help businesses reduce content creation costs, improve consistency, and increase productivity.\n",
       "2. **Chatbots and Virtual Assistants**: Generative AI enables the creation of conversational interfaces that can understand natural language and respond accordingly. Chatbots can be used for customer support, sales, and marketing automation.\n",
       "3. **Image and Video Generation**: AI-powered tools can generate high-quality images and videos, which can be used in various applications such as advertising, product visualization, and entertainment.\n",
       "4. **Data Augmentation**: Generative AI can be used to augment existing datasets, generating new data that is similar in style and structure to the original data. This can help improve machine learning model performance and reduce the need for large amounts of labeled data.\n",
       "5. **Predictive Maintenance**: AI-powered predictive maintenance tools use generative models to predict equipment failures and schedule maintenance accordingly. This can help businesses reduce downtime, improve efficiency, and extend equipment lifespan.\n",
       "6. **Personalized Recommendations**: Generative AI algorithms can analyze customer behavior and preferences to generate personalized product recommendations, improving the overall user experience and increasing sales.\n",
       "7. **Design Assistance**: AI-powered design tools can assist architects, engineers, and designers in generating new designs, reducing the need for manual prototyping, and improving collaboration between teams.\n",
       "8. **Market Research**: Generative AI can analyze large datasets to identify trends, patterns, and insights that can inform business decisions and market strategies.\n",
       "9. **Product Development**: AI-powered tools can generate ideas for new products, improve product design, and even create prototypes, accelerating the product development process.\n",
       "10. **Marketing Automation**: Generative AI algorithms can analyze customer data and behavior to generate targeted marketing campaigns, improving campaign effectiveness and ROI.\n",
       "\n",
       "Some specific business applications of generative AI include:\n",
       "\n",
       "* **Fashion**: Designing clothing and accessories, predicting fashion trends, and generating personalized product recommendations.\n",
       "* **Finance**: Generating investment ideas, predicting market trends, and optimizing portfolio performance.\n",
       "* **Healthcare**: Developing personalized treatment plans, analyzing medical images, and identifying new disease patterns.\n",
       "* **Retail**: Creating personalized product recommendations, generating customer personas, and improving supply chain optimization.\n",
       "* **Manufacturing**: Predicting equipment failures, optimizing production schedules, and generating optimized manufacturing processes.\n",
       "\n",
       "These are just a few examples of the many business applications of generative AI. As the technology continues to evolve, we can expect to see even more innovative applications across various industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the meanings of \"core concepts\" related to Large Language Models (LLMs) that include \"neural network,\" \"attention,\" and the \"transformer.\" Let me start by understanding what each term means on its own and then see how they fit into an LLM.\n",
      "\n",
      "First, a neural network. I know it's something with layers of nodes called neurons. They process input data in stages where each layer processes information from the previous one. They can learn patterns and make predictions or classifications based on data.\n",
      "\n",
      "Next is attention. From what I remember, attention mechanisms let models focus on particular parts of the text when processing each word. So, perhaps attention isn't just about looking at words but also determining which ones are most relevant during language understanding.\n",
      "\n",
      "Now, Transformers. They sound familiar because they were a big deal in AI. Wait, the user asked for LLMs, so maybe it's referring to \"large\" transformers since these models typically refer to their structure or capacity with the Transformer component. So the key concepts around Transformers would be something like self-attention and positional encoding.\n",
      "\n",
      "Putting this together: A neural network is the foundation that processes input data through layers of nodes. Attention might be an internal feature of the network, using the Transformer's mechanisms. The core concept in LLMs related to the Transformer might be self-attention, which allows models to focus on different parts of text for understanding.\n",
      "\n",
      "Wait, I'm not sure if \"computationally intensive\" is a core concept within Transformers. Or maybe it's more about how many parameters they have or other technical aspects. I should make sure those terms are accurate and correct.\n",
      "</think>\n",
      "\n",
      "**Core Concepts in LLMs:**\n",
      "\n",
      "1. **Neural Network:** A neural network consists of interconnected layers of nodes, where each layer processes information sequentially. Nodes process input data through transformations that mimic biological neurons.\n",
      "\n",
      "2. **Attention Mechanisms:** Attention enables models to focus on specific parts of text by allowing the model to recognize which words relate to any given word during processing.\n",
      "\n",
      "3. **Transformer Architecture:** The Transformer refers implicitly or explicitly through self-attention and positional encoding, enabling models to process sequential data efficiently without relying on fixed-size vocabularies.\n",
      "\n",
      "So, for an LLM, a neural network processes input data, attention enhances this processing by identifying relevant words, while Transformers use self-attention and positional encoding to handle text effectively.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too. Explain what you do, e.g. thinking\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42a5645a-6652-4bfe-9ffa-e87b8b9b323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cda605ea-d720-4318-ac09-942d91432f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url, model=MODEL):\n",
    "    website = Website(url)\n",
    "    \n",
    "    response = ollama_via_openai.chat.completions.create(model=MODEL, messages=messages_for(website), verbosity=True)\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    return display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "955a4d87-6942-4c8b-b372-af7058a8f0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Short Summary of Ollama Website**\n",
       "=====================================\n",
       "\n",
       "The website `Ollama` appears to be a platform focused on artificial intelligence and machine learning models.\n",
       "\n",
       "*   **Cloud Models:** The website announces that its cloud-based models are now available for download, with features such as chat and building capabilities. These models can be accessed by users on macOS, Windows, or Linux operating systems.\n",
       "*   **Models Categories:**\n",
       "    *   `Cloud models` is the main announcement about cloud-based models being available.\n",
       "    *   \"Explore models\" suggests that there are various pre-trained models available for users to experiment with and download.\n",
       "\n",
       "### **News/Announcements**\n",
       "\n",
       "We can confirm that:\n",
       "\n",
       "*   The website has updated its cloud models feature, making it more user-friendly and accessible on multiple operating systems.\n",
       "*   Users can now explore pre-trained models categorized under \"Explore models\" or by downloading the cloud-based models through Ollama.\n",
       "\n",
       "Note: No other announcements were found beyond the cloud model availability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize(\"https://ollama.com/\", model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f1d58-f265-4cb6-8b94-61f9cb90eb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
